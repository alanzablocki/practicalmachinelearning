---
title: "Practical Machine Learning Project"
author: "Alan Zablocki"
date: "June 15, 2016"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setting up

We load the required libraries.
```{r}
library(caret)
library(randomForest)
library(rattle)
library(rpart)
library(rpart.plot)
library(corrplot)
```
Load local data.
```{r}
training = read.csv("pml-training.csv")
testing = read.csv("pml-testing.csv")
```
Check the dimensions.
```{r}
dim(training)
dim(testing)
```
Find out how many NA values using complete.cases()
```{r}
sum(complete.cases(training))
```
## Cleaning the data

We will remove columsn with NA values, columns with data that is not needed for training,e.g., names and time stamps etc.

We remove columsn with NA values like this:
```{r}
train_cleaned <- training[, colSums(is.na(training)) == 0]
```
There are seven columns at the beginning with information we do not need
```{r}
sum(grepl("^X|user_name|timestamp|window",names(train_cleaned)))
remove_train_col <- grepl("^X|user_name|timestamp|window",names(train_cleaned))
train_cleaned <- train_cleaned[,!remove_train_col]
ncol(train_cleaned)
```
We are left with 86 columns but some have no values, and so we remove those too. Our "classe" variable will be removed but we column bind it back and make sure it is still called "class". 
```{r}
trClean <- train_cleaned[, sapply(train_cleaned, is.numeric)]
trained <- cbind(trClean,train_cleaned[,"classe"])
names(trained)[53] <- "classe"
colnames(trained)
```
We also check if any of the variables have near zero variance.
```{r}
mydata <- nearZeroVar(trained, saveMetrics = TRUE)
mydata
```
Since all the variables return FALSE, we are left with 52 variables to build our model. Before this we apply the same cleaning procedures to the final testing dataset with 20 cases.

```{r}
test_cleaned <- testing[, colSums(is.na(testing)) == 0]
remove_test_col <- grepl("^X|user_name|timestamp|window",names(test_cleaned))
test_cleaned <- test_cleaned[,!remove_test_col]
testClean <- test_cleaned[, sapply(test_cleaned, is.numeric)]
dim(testClean)
dim(trClean)
```
We should also make sure the training and test datasets have same
columns/names.
```{r}
names(test_cleaned) == names(trained)
```
Last column names is different, but that is OK. 

## Building predictions
We split data intro training and testing samples, and set a unique seed
so we can reproduce the results. We will use 65 35 split for training and testing.

```{r}
set.seed(83912)
inTrain <- createDataPartition(y=trained$clas,p=0.65, list=FALSE)
my_training <- trained[inTrain,]
my_testing <- trained[-inTrain,]
```

### Predicting with decision trees
```{r}
model_Dtree <- rpart(classe ~.,data = my_training, method = "class")
prediction_Dtree <- predict(model_Dtree, my_testing, type = "class")
confusionMatrix(prediction_Dtree, my_testing$classe)
model_Dtree
```
I find that decision trees are not good enough to get the sort of accuracy we are after. Therefore I will try random forests now.
### Predicting with random forests
```{r}
model_RF1 <- randomForest(classe ~., data = my_training)
prediction_RF1 <- predict(model_RF1, my_testing, type = "class")
confusionMatrix(prediction_RF1, my_testing$classe)
model_RF1
```
When running RF with 500 trees we get a very good accuracy of about 99.4 percent. I find that this does not vary much if we reduce the number of trees to 100 (Acc = 0.9932). Adding k-fold cross-validation does not improve the result. Pre-processing (centering and scaling) also does not change the result.
### Adding cross validation
```{r}
control <- trainControl(method = "cv",5)
model_RF2 <- train(classe ~., data = my_training, method = "rf",trControl=control, ntree=100)
prediction_RF2 <- predict(model_RF2, my_testing)
confusionMatrix(prediction_RF2, my_testing$classe)
model_RF2
```
I can work out the accuracy and out of sample error on the RF1 model, which I also apply to the final test set.
```{r}
accuracy <- postResample(prediction_RF1, my_testing$classe)
accuracy
OutSampleErr <- 1- as.numeric(confusionMatrix(prediction_RF1, my_testing$classe)$overall[1])
OutSampleErr
```
Running this on the test set for the quiz
```{r}
fin_results <- predict(model_RF1,test_cleaned[,-length(names(test_cleaned))])
fin_results
```
# Plots
```{r}
plot(model_RF1)
```
## Correlation matrix
```{r}
corrPlot <- cor(my_training[, -length(names(my_training))])
corrplot(corrPlot, method="color")
```
## Decision Tree
```{r}
prp(model_Dtree)
fancyRpartPlot(model_Dtree)
```
